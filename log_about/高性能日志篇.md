### 前言

日志作为我们开发过程中最为基础的一个模块，也是最常用的手段，存在以下问题需要我们解决。

* 日志丢失问题
* 写入IO性能瓶颈
* 序列化瓶颈
* 敏感数据加密
* 数据压缩
* 其他问题

所以，如何打造一个高性能，高可用的日志基础组件也是一个具有挑战性的事情。

### 日志丢失和IO瓶颈

#### 日志为什么会丢失？

当写文件的时候，并不是把数据直接写入了磁盘，而是先把数据写入到系统的缓存(dirty page)中，系统一般会在下面几种情况把 dirty page 写入到磁盘：

* 定时回写，相关变量在/proc/sys/vm/dirty_writeback_centisecs和/proc/sys/vm/dirty_expire_centisecs中定义。
* 调用 write 的时候，发现 dirty page 占用内存超过系统内存一定比例，相关变量在/proc/sys/vm/dirty_background_ratio（ 后台运行不阻塞 write）和/proc/sys/vm/dirty_ratio（阻塞 write）中定义。
* 内存不足。

因此，在一些特定情况下，日志来不及回写到磁盘，就会发生 日志丢失的现象。

#### 为什么会产生IO瓶颈呢？

数据从程序写入到磁盘的过程中，其实牵涉到两次数据拷贝：一次是用户空间内存拷贝到内核空间的缓存，一次是回写时内核空间的缓存到硬盘的拷贝。当发生回写时也涉及到了内核空间和用户空间频繁切换。 
dirty page 回写的时机对应用层来说又是不可控的，所以性能瓶颈就出现了。


#### 如何解决问题

mmap，关于mmap的优势和用法，网上有很多介绍，而且现在也有很多基于mmap实现的高性能日志，因此这里不做过多的介绍。

Java也可以使用mmap，为什么要使用c++去写呢？主要是为了 端上统一，省去代码维护成本。

### 序列化瓶颈

通常，我们会将日志序列化为Json，但是，序列化Json的过程也是一个耗时的操作。尤其是对于Gson这样的库。那么，我们如何才能提高序列化效率呢。

* 将Json的序列化过程放到C++做
* 采用更高性能的序列化手段，例如:protobuf


### 加密

加密这块，我们只要使用常规的非对称加密或者其他方式进行加密既可。

### 压缩 

压缩这块，我们可以选择开源的zlib或者gzip等等处理、

